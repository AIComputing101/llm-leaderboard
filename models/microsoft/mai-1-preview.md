# MAI-1-preview

> Large Language Model

**Organization:** [Microsoft](../../labs/microsoft.md)
**Released:** Aug/2025
**Access:** ğŸŸ¢ Public

---

## ğŸ“Š Overview

MAI-1-preview is a large language model developed by Microsoft.

## ğŸ”§ Technical Specifications

### Model Architecture
- **Type:** MoE
- **Parameters:** 500.0B
- **Training Tokens:** 10000.0B
- **Token:Param Ratio:** 20:1 âœ… Compute-optimal

### Training Efficiency
- **ALScore:** 7.5 (Powerful)
- **Formula:** âˆš(Parameters Ã— Tokens) Ã· 300

### Training Data
- **Dataset Type:** synthetic, web-scale

## ğŸ“ˆ Performance Benchmarks

| Benchmark | Score | Description |
|-----------|-------|-------------|
| **MMLU** | - | General knowledge across 57 subjects |
| **MMLU-Pro** | - | Advanced MMLU variant |
| **GPQA** | - | Graduate-level reasoning |
| **HLE** | - | High-level evaluation |

## ğŸ·ï¸ Model Tags

_No specific tags_

## ğŸ“ Additional Notes

MAI=Microsoft artificial intelligence. "MAIâ€™s first foundation model trained end-to-end... MAI-1-preview is an in-house mixture-of-experts model, pre-trained and post-trained on ~15,000 NVIDIA H100 GPUs. This model is designed to provide powerful capabilities to consumers seeking to benefit from models that specialize in following instructions and providing helpful responses to everyday queries. We will be rolling MAI-1-preview out for certain text use cases within Copilot"

## ğŸ”— Resources

- ğŸ® [Try the Model](https://microsoft.ai/news/two-new-in-house-models/)
- ğŸ“„ [Technical Documentation](https://microsoft.ai/news/two-new-in-house-models/)

## ğŸ” Related Models

**From Microsoft:**
- See all models in [Microsoft profile](../../labs/microsoft.md)

**Similar Architecture:**
- See all [MoE models](../../architectures/moe.md)

---

**Last Updated:** 2025-10-02

[â† Back to Leaderboard](../../README.md) â€¢ [View All Microsoft Models](../../labs/microsoft.md)
