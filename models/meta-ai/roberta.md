# RoBERTa

> Large Language Model

**Organization:** [Meta AI](../../labs/meta-ai.md)
**Released:** Jul/2019
**Access:** ğŸŸ¢ Public

---

## ğŸ“Š Overview

RoBERTa is a large language model developed by Meta AI.

## ğŸ”§ Technical Specifications

### Model Architecture
- **Type:** Dense
- **Parameters:** 0.355B
- **Training Tokens:** 2200.0B
- **Token:Param Ratio:** 6,198:1 âœ… Compute-optimal

### Training Efficiency
- **ALScore:** 0.1 (Lightweight)
- **Formula:** âˆš(Parameters Ã— Tokens) Ã· 300

### Training Data
- **Dataset Type:** web-scale

## ğŸ“ˆ Performance Benchmarks

| Benchmark | Score | Description |
|-----------|-------|-------------|
| **MMLU** | 27.9 | General knowledge across 57 subjects |
| **MMLU-Pro** | - | Advanced MMLU variant |
| **GPQA** | - | Graduate-level reasoning |
| **HLE** | - | High-level evaluation |

## ğŸ·ï¸ Model Tags

_No specific tags_

## ğŸ“ Additional Notes

calcs: "In total, this batch size and number of steps corresponds to pre-training on 235 â‰ˆ 34B tokens. This is considerably less than BERT (Devlin et al., 2018), which used roughly 137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly 2.2T tokens. Using only 2 35 tokens results in a reasonable computational budget while still providing a sufficient amount of pre-training for acceptable performance. We consider the effect of pre-training for more steps in Sections 3.6 and 3.7. Note that 2 35 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training." https://arxiv.org/pdf/1910.10683.pdf MMLU shows RoBERTa-base 125M only=27.9 (not 355M)

## ğŸ”— Resources

- ğŸ® [Try the Model](https://huggingface.co/FacebookAI/roberta-large)
- ğŸ“„ [Technical Documentation](https://arxiv.org/abs/1907.11692)

## ğŸ” Related Models

**From Meta AI:**
- See all models in [Meta AI profile](../../labs/meta-ai.md)

**Similar Architecture:**
- See all [Dense models](../../architectures/dense.md)

---

**Last Updated:** 2025-10-02

[â† Back to Leaderboard](../../README.md) â€¢ [View All Meta AI Models](../../labs/meta-ai.md)
